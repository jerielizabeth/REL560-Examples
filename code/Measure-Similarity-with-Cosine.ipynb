{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Measuring Similarity Between Documents\n",
    "\n",
    "We are going to measure similarity between documents by representing them as a vector of their most significant words and then measuring the distance beween those vectors.\n",
    "\n",
    "So, we are going to represent our documents in a format called [TF-IDF using a library called Scikit Learn](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html?highlight=tfidf#sklearn.feature_extraction.text.TfidfVectorizer).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code examples from \n",
    "- https://goodboychan.github.io/python/datacamp/natural_language_processing/2020/07/17/04-TF-IDF-and-similarity-scores.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install to the REL 560 Environment\n",
    "- Pandas\n",
    "- Numpy\n",
    "- SciKit Learn \n",
    "- Matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load texts into a dataframe\n",
    "\n",
    "source_dir = \"../data/Example_texts/history/NYT-Obituaries/\"\n",
    "\n",
    "texts = []\n",
    "\n",
    "for filename in os.listdir(source_dir):\n",
    "    with open(os.path.join(source_dir, filename), 'r') as obit:\n",
    "        content = obit.read()\n",
    "    texts.append(\n",
    "        {\n",
    "            \"doc_id\": filename,\n",
    "            \"text\": content\n",
    "        }\n",
    "    )\n",
    "\n",
    "texts_df = pd.DataFrame(texts)\n",
    "\n",
    "texts_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_df.reset_index(inplace=True)\n",
    "texts_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Tf-IDF Vector Representation\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Generate matrix of word vectors\n",
    "tfidf_matrix = vectorizer.fit_transform(texts_df['text'])\n",
    "\n",
    "print(tfidf_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)\n",
    "print(cosine_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_df = pd.DataFrame(cosine_sim)\n",
    "corr_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "corr_df.style.background_gradient(cmap ='viridis')\\\n",
    "        .set_properties(**{'font-size': '8px'})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the correspondence, we can do a couple of things:\n",
    "- Get most similar documents\n",
    "- Get least similar documents\n",
    "- Get documents most similar to a particular document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = corr_df.unstack().reset_index()\n",
    "# pairs_df.columns = ['doc_A', 'doc_B', 'similarity_measure']\n",
    "pairs_df = pd.DataFrame(pairs)\n",
    "pairs_df.columns = ['Doc_A', 'Doc_B', 'Similarity_Score']\n",
    "\n",
    "pairs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean out rows where matching self\n",
    "\n",
    "pairs_df = pairs_df[pairs_df['Doc_A'] != pairs_df['Doc_B']]\n",
    "pairs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/48549637/pandas-removing-mirror-pairs-from-dataframe\n",
    "#  df.loc[pd.DataFrame(np.sort(df[['A','B']],1),index=df.index).drop_duplicates(keep='first').index]\n",
    "\n",
    "unique_pairs = pairs_df.loc[pd.DataFrame(np.sort(pairs_df[['Doc_A', 'Doc_B']], 1), index=pairs_df.index).drop_duplicates(keep='first').index]\n",
    "\n",
    "unique_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get most similar documents\n",
    "\n",
    "def get_top_docs(sim_df, metadata, num_docs=10, rank='top'):\n",
    "    sorted = sim_df.sort_values('Similarity_Score', ascending=True)\n",
    "    if rank == 'top':\n",
    "        sliced = sorted.tail(num_docs)\n",
    "    elif rank == 'bottom':\n",
    "        sliced = sorted.head(num_docs)\n",
    "    else:\n",
    "        return \"Please use 'top' or 'bottom' for rank variable\"\n",
    "    \n",
    "    sliced_named = sliced.merge(metadata, how=\"left\", left_on = \"Doc_A\", right_on=\"index\").merge(metadata, how=\"left\", left_on=\"Doc_B\", right_on=\"index\")\n",
    "    sliced_named.columns = ['Doc_A', 'Doc_B', 'Similarity_Score', 'Index', 'Doc_A_ID', 'IndexB', 'Doc_B_ID']\n",
    "\n",
    "    # print(sliced_named)\n",
    "\n",
    "    top_docs_df = sliced_named[['Similarity_Score', 'Doc_A_ID', 'Doc_B_ID']]\n",
    "\n",
    "    return top_docs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_top_docs(unique_pairs, texts_df[['index', 'doc_id']], rank='top')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_similar_docs(title, sim_mx, metadata):\n",
    "\n",
    "    idx = metadata.index[metadata['doc_id'] == title].tolist()\n",
    "    # print(idx)\n",
    "\n",
    "    # Get similarity scores\n",
    "    sim_scores = list(enumerate(sim_mx[idx[0]]))\n",
    "\n",
    "    # sort them\n",
    "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Get the top 10 (ignorning the 1 for self-matching)\n",
    "\n",
    "    sim_scores = sim_scores[1:11]\n",
    "    \n",
    "    scores = pd.DataFrame(sim_scores)\n",
    "    scores.columns = ['index', 'similarity_score']\n",
    "    # print(scores)\n",
    "\n",
    "    title_index = [i[0] for i in sim_scores]\n",
    "    matches = pd.DataFrame(metadata['doc_id'].iloc[title_index]).reset_index()\n",
    "    # print(matches)\n",
    "    matches = matches.merge(scores, how=\"left\", on=\"index\")\n",
    "\n",
    "    return matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_similar_docs('1870-Robert-E-Lee.txt', corr_df, texts_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "bcdea5b07817f9f45b1f08b45d2d914798d13871639a06d7496ef308c7e8062c"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
